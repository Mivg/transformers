--model_name_or_path bert-base-uncased --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train --do_eval --output_dir /tmp/test-mlm --data_cache_dir /media/disk2/maori/netapp/data/wiki

--output_dir /media/disk2/maori/netapp/outputs/models/wiki/test_mini_bert
--model_type bert
--mlm
--config_name config.json
--tokenizer_name bert-base-uncased
--do_train
--do_eval
--learning_rate 1e-4
--num_train_epochs 5
--save_total_limit 2
--save_steps 2000
--per_gpu_train_batch_size 16
--evaluate_during_training
--seed 42
--cache_dir /media/disk2/maori/netapp/cache
--dataset_name wikipedia
--dataset_config_name 20200501.en
--data_cache_dir /media/disk2/maori/netapp/data/wiki


--output_dir /media/disk2/maori/netapp/outputs/models/wiki/test_mini_bert
--config_name config.json
--tokenizer_name bert-base-uncased
--do_train
--do_eval
--learning_rate 1e-4
--num_train_epochs 5
--save_total_limit 2
--save_steps 2000
--per_gpu_train_batch_size 16
--seed 42
--cache_dir /media/disk2/maori/netapp/cache
--dataset_name wikipedia
--dataset_config_name 20200501.en
--data_cache_dir /media/disk2/maori/netapp/data/wiki



--output_dir /media/disk2/maori/netapp/outputs/models/wiki/test_mini_bert --config_name ../../configs/test_mini_bert_config.json --tokenizer_name bert-base-uncased --do_train --do_eval --learning_rate 1e-4 --num_train_epochs 5 --save_total_limit 2 --save_steps 2000 --per_gpu_train_batch_size 16 --seed 42 --cache_dir /media/disk2/maori/netapp/cache --dataset_name wikipedia --dataset_config_name 20200501.en --data_cache_dir /media/disk2/maori/netapp/data/wiki --preprocessing_num_workers 16

from training_args.py:
per_device_train_batch_size
per_device_eval_batch_size
gradient_accumulation_steps
max_steps "If > 0: set total number of training steps to perform. Override num_train_epochs."
save_steps: "Save checkpoint every X updates steps." (default 500)
save_total_limit: "Deletes the older checkpoints in the output_dir. Default is unlimited checkpoints"
eval_steps: "Run an evaluation every X steps."
dataloader_num_workers: "Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the main process."
logging_steps

--output_dir /media/disk2/maori/netapp/outputs/models/wiki/mbert_1_32_4 --config_name ../../configs/mbert_1_32_4.json --tokenizer_name bert-base-uncased --do_train --do_eval --learning_rate 1e-4 --num_train_epochs 5 --max_steps 500000 --save_total_limit 15 --save_steps 10000 --per_gpu_train_batch_size 256 --per_gpu_eval_batch_size 1024 --seed 42 --cache_dir /media/disk2/maori/netapp/cache --dataset_name wikipedia --dataset_config_name 20200501.en --data_cache_dir /media/disk2/maori/netapp/data/wiki --preprocessing_num_workers 16 --dataloader_num_workers 4 --logging_steps 500